{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "from VAE import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare loss functions\n",
    "\n",
    "# def log_bernoulli_loss(x_hat, x):\n",
    "#     x = torch.unsqueeze(x, 1)\n",
    "#     x_hat = torch.transpose(torch.unsqueeze(x_hat, 1), 1, 2)\n",
    "#     loss = torch.bmm(x, torch.log(x_hat))\n",
    "#     loss += torch.bmm(1 - x, torch.log(1 - x_hat))\n",
    "#     return -torch.sum(loss)\n",
    "\n",
    "def log_bernoulli_loss(x_hat, x):\n",
    "    loss = torch.sum(x * torch.log(x_hat) + (1-x) *(torch.log(1-x_hat)), 1)\n",
    "    return - torch.sum(loss)\n",
    "\n",
    "# def KL_loss(mu, logvar):\n",
    "#     _, D = mu.size()\n",
    "#     var = torch.exp(logvar)\n",
    "#     trace = torch.sum(var, dim=1)\n",
    "#     logsum = torch.sum(logvar, dim=1)\n",
    "#     mu = torch.unsqueeze(mu, 1)\n",
    "#     mu_hat = torch.transpose(mu, 1, 2)\n",
    "#     loss = 0.5 * (trace + torch.bmm(mu, mu_hat).squeeze() - logsum - D)\n",
    "#     return torch.sum(loss)\n",
    "\n",
    "def KL_loss(mu, logvar):\n",
    "    # Gaussian\n",
    "    D = torch.FloatTensor([mu.size(1)])\n",
    "    log_D = torch.log(D)\n",
    "    sum_logvar = torch.sum(logvar, 1)\n",
    "    norm_var = torch.sum(torch.exp(logvar), 1)\n",
    "    norm_mu = torch.sum(mu * mu, 1)\n",
    "    loss = (log_D - sum_logvar + norm_var + norm_mu - D)/2\n",
    "    return torch.sum(loss)\n",
    "\n",
    "def loss_function(x_hat, x, mu, logvar):\n",
    "    return log_bernoulli_loss(x_hat, x) + KL_loss(mu, logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, train_loader, model, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data.view(-1, 784), mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set learning rate, batch size and number of epochs\n",
    "\n",
    "lr = 1e-3\n",
    "batch_size = 20\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size=batch_size, shuffle=True, **{})\n",
    "\n",
    "# Init model\n",
    "VAE_MNIST = VAE(784)\n",
    "\n",
    "# Init optimizer\n",
    "optimizer = optim.Adam(VAE_MNIST.parameters(), lr=lr)\n",
    "\n",
    "# Train\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch, train_loader, VAE_MNIST, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's check if the reconstructions make sense\n",
    "# Set model to test mode\n",
    "VAE_MNIST.eval()\n",
    "    \n",
    "# Reconstructed\n",
    "train_data_plot = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor())\n",
    "\n",
    "train_loader_plot = torch.utils.data.DataLoader(train_data_plot,\n",
    "                                           batch_size=1, shuffle=False, **{})\n",
    "\n",
    "for batch_idx, (data, _) in enumerate(train_loader_plot):\n",
    "    x_hat, mu, logvar = VAE_MNIST(data)\n",
    "    plt.imshow(x_hat.view(1,28,28).squeeze().data.numpy(), cmap='gray')\n",
    "    plt.title('%i' % train_data.train_labels[batch_idx])\n",
    "    plt.show()\n",
    "    if batch_idx == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
