{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# declare loss functions\n",
    "\n",
    "def log_bernoulli_loss(x_hat, x):\n",
    "    \n",
    "    loss = - torch.sum(x * torch.log(x_hat) + (1-x) *(torch.log(1-x_hat)), 1)\n",
    "    \n",
    "    return torch.sum(loss)\n",
    "\n",
    "\n",
    "def KL_loss(mu, logvar):\n",
    "    \n",
    "    D = torch.FloatTensor([mu.size(1)])\n",
    "    log_D = torch.log(D)\n",
    "    sum_logvar = torch.sum(logvar, 1)\n",
    "    norm_var = torch.sum(torch.exp(logvar), 1)\n",
    "    norm_mu = torch.sum(mu * mu, 1)\n",
    "    loss = (log_D - sum_logvar + norm_var + norm_mu - D)/2\n",
    "    \n",
    "    return torch.sum(loss)\n",
    "\n",
    "\n",
    "def loss_function(x_hat, x, mu, logvar):\n",
    "    \n",
    "    log_bernoulli = log_bernoulli_loss(x_hat, x)\n",
    "    KL = KL_loss(mu, logvar)\n",
    "    \n",
    "    return log_bernoulli+KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F \n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, fc1_dims, fc21_dims, fc22_dims, fc3_dims, fc4_dims):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(*fc1_dims)\n",
    "        self.fc21 = nn.Linear(*fc21_dims)\n",
    "        self.fc22 = nn.Linear(*fc22_dims)\n",
    "        self.fc3 = nn.Linear(*fc3_dims)\n",
    "        self.fc4 = nn.Linear(*fc4_dims)\n",
    "\n",
    "    def encode(self, x):\n",
    "        embedding = F.relu(self.fc1(x))\n",
    "        mu = F.sigmoid(self.fc21(embedding))\n",
    "        logvar = F.tanh(self.fc22(embedding))\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        epsilon = torch.normal(torch.zeros(logvar.size()), torch.ones(logvar.size()))\n",
    "        sigma = torch.sqrt(torch.exp(logvar))\n",
    "        z = mu + epsilon * logvar\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        x_hat =  F.sigmoid(self.fc4(self.fc3(z)))\n",
    "        return x_hat\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, train_loader, model, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data.view(-1, 784), mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "fc1_dims = (784,400)\n",
    "fc21_dims = (400,20)\n",
    "fc22_dims = (400,20)\n",
    "fc3_dims = (20, 400)\n",
    "fc4_dims = (400,784)\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 550.830078\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 211.150452\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 170.457962\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 157.447418\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 167.667160\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 147.450348\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 136.125763\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 128.148544\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 142.303574\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 129.733139\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 121.319611\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 116.309731\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 129.097626\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 107.516060\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 113.921371\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 110.488884\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 115.985474\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 109.106956\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 113.194550\n",
      "====> Epoch: 1 Average loss: 137.8186\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 112.637512\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 114.724823\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 104.121529\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 107.980209\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 108.974525\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 95.279015\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 95.556366\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 100.097153\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 96.676453\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 104.929131\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 108.249733\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 96.085358\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 107.999321\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 100.437943\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 109.250572\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 93.224121\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 93.151833\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 95.018982\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 96.013214\n",
      "====> Epoch: 2 Average loss: 100.6542\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 98.984909\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 98.392746\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 93.372375\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 90.304512\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 95.471581\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 86.880295\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 93.715485\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 94.503456\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 95.953789\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 91.964157\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 84.190163\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 87.473755\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 88.867218\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 89.889214\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 96.829742\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 94.770409\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 95.225288\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 91.497520\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 90.288795\n",
      "====> Epoch: 3 Average loss: 91.7727\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 85.085052\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 93.646591\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 90.077507\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 88.749672\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 93.064003\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 88.674629\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 90.932716\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 87.018761\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 81.761063\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 87.870827\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 93.555611\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 88.290146\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 90.160751\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 88.962044\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 90.873848\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 94.689789\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 89.217407\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 93.267929\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 94.187546\n",
      "====> Epoch: 4 Average loss: 88.3313\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 90.895752\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 79.044418\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 81.448914\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 87.259377\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 90.000397\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 91.181725\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 80.196800\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 93.185928\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 88.896301\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 85.194725\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 93.835419\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 97.718597\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 89.454590\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 90.034546\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 91.141777\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 78.902824\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 89.583275\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 86.831924\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 82.318802\n",
      "====> Epoch: 5 Average loss: 86.8586\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 81.586609\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 89.050194\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 84.372459\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 85.968910\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 90.059082\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 83.536285\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 78.288361\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 80.488510\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 84.977539\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 94.041962\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 96.771767\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 77.083321\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 81.241730\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 84.028885\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 88.755989\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 79.582146\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 87.440971\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 82.068886\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 85.811226\n",
      "====> Epoch: 6 Average loss: 86.0654\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 82.770851\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 83.911957\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 84.637314\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 84.836845\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 85.066528\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 84.632332\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 78.115509\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 89.863441\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 85.738518\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 76.371567\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 80.473320\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 87.455315\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 79.510246\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 93.195503\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 86.020760\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 86.841492\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 76.251289\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 89.686676\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 81.794067\n",
      "====> Epoch: 7 Average loss: 85.5360\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 80.437286\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 86.705269\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 83.934624\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 89.643051\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 83.420929\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 83.283226\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 86.448013\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 80.164665\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 83.404381\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 86.750168\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 79.951759\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 87.458099\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 82.065941\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 87.852661\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 86.784416\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 86.493889\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 94.334343\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 79.407982\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 89.561974\n",
      "====> Epoch: 8 Average loss: 85.0949\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 91.980652\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 75.891449\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 79.802231\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 84.854881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 82.419060\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 74.994934\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 86.295601\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 82.733406\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 92.272247\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 86.362236\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 86.960068\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 78.461273\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 88.502586\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 85.809181\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 75.637283\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 88.663155\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 86.481720\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 82.719078\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 88.493736\n",
      "====> Epoch: 9 Average loss: 84.8151\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 92.469582\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 86.385712\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 88.447319\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 80.475220\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 90.774429\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 83.975960\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 91.017136\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 81.513916\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 84.932632\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 82.142998\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 81.315445\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 89.255386\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 89.751732\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 89.215469\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 76.519638\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 91.798599\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 84.765915\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 84.486397\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 90.822220\n",
      "====> Epoch: 10 Average loss: 84.5317\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_data = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size=batch_size, shuffle=True, **{})\n",
    "\n",
    "# Init model\n",
    "VAE_MNIST = VAE(fc1_dims=fc1_dims, fc21_dims=fc21_dims, fc22_dims=fc22_dims, fc3_dims=fc3_dims, fc4_dims=fc4_dims)\n",
    "\n",
    "# Init optimizer\n",
    "optimizer = optim.Adam(VAE_MNIST.parameters(), lr=lr)\n",
    "\n",
    "# Train\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch, train_loader, VAE_MNIST, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Let's check if the reconstructions make sense\n",
    "# Set model to test mode\n",
    "VAE_MNIST.eval()\n",
    "    \n",
    "# Reconstructed\n",
    "train_data_plot = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor())\n",
    "\n",
    "train_loader_plot = torch.utils.data.DataLoader(train_data_plot,\n",
    "                                           batch_size=1, shuffle=False, **{})\n",
    "\n",
    "for batch_idx, (data, _) in enumerate(train_loader_plot):\n",
    "    x_hat, mu, logvar = VAE_MNIST(data)\n",
    "    plt.imshow(x_hat.view(1,28,28).squeeze().data.numpy(), cmap='gray')\n",
    "    plt.title('%i' % train_data.train_labels[batch_idx])\n",
    "    plt.show()\n",
    "    if batch_idx == 3:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
