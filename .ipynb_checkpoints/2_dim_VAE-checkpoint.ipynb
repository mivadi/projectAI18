{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project AI\n",
    "\n",
    "Import required sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "from VAE import *\n",
    "from train import *\n",
    "import numpy as np\n",
    "from collections import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Gaussian model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "latent_dim = 2\n",
    "batch_size = 50\n",
    "epochs = 50\n",
    "\n",
    "# Load data\n",
    "train_data = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VAE_Gaussian, loss_Gaussian, z_Gaussian, KL_Gaussian, log_bern_Gaussian = run_train(latent_dim, epochs, 'Gaussian', train_data, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VAE_gumbel, loss_gumbel, z_gumbel, KL_gumbel, log_bern_gumbel = run_train(latent_dim, epochs, 'Gumbel', train_data, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VAE_logit, loss_logit, z_logit, KL_logit, log_bern_logit = run_train(latent_dim, epochs, 'logit', train_data, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(520.9139)\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 520.913879\n",
      "tensor(437.6595)\n",
      "tensor(395.7644)\n",
      "tensor(339.6401)\n",
      "tensor(295.6259)\n",
      "tensor(263.9840)\n",
      "tensor(222.7001)\n",
      "tensor(193.5430)\n",
      "tensor(183.6715)\n",
      "tensor(163.2850)\n",
      "tensor(156.6894)\n",
      "tensor(127.2094)\n",
      "tensor(154.4530)\n",
      "tensor(161.1390)\n",
      "tensor(130.0685)\n",
      "tensor(137.4625)\n",
      "tensor(133.4250)\n",
      "tensor(130.8753)\n",
      "tensor(128.3878)\n",
      "tensor(159.2956)\n",
      "tensor(145.7984)\n",
      "tensor(154.4469)\n",
      "tensor(148.8844)\n",
      "tensor(124.5636)\n",
      "tensor(145.1872)\n",
      "tensor(138.0462)\n",
      "tensor(136.8770)\n",
      "tensor(144.4399)\n",
      "tensor(135.9601)\n",
      "tensor(118.1170)\n",
      "tensor(158.0745)\n",
      "tensor(133.2163)\n",
      "tensor(137.3362)\n",
      "tensor(119.0384)\n",
      "tensor(126.1679)\n",
      "tensor(122.0036)\n",
      "tensor(131.6362)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-34f44a156071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mVAE_logit_rank1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_logit_rank1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_logit_rank1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKL_logit_rank1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_bern_rank1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'logit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/petru/Desktop/secondPAI/projectAI18/train.py\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(latent_dim, epochs, method, train_data, lr, rank1, variance)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         average_loss, average_KL_loss, average_log_bernoulli_loss, z = train(\n\u001b[0;32m---> 86\u001b[0;31m             epoch, train_loader, model, optimizer)\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mall_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mKL_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_KL_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/petru/Desktop/secondPAI/projectAI18/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader, model, optimizer)\u001b[0m\n\u001b[1;32m     21\u001b[0m             data.view(-1, 784), recon_batch, z, z_parameters)\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mKL_losses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mKL_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random.seed(1000)\n",
    "\n",
    "VAE_logit_rank1, loss_logit_rank1, z_logit_rank1, KL_logit_rank1, log_bern_rank1 = run_train(latent_dim, epochs, 'logit', train_data, 1e-3, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VAE_logit_sigmoid, loss_logit_sigmoid, z_logit_sigmoid, KL_logit_sigmoid, log_bern_sigmoid = run_train(3, epochs, 'logit-sigmoidal', train_data, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch_space = np.linspace(1,50,50)\n",
    "plt.plot(epoch_space, loss_Gaussian, label='Gaussian')\n",
    "plt.plot(epoch_space, loss_logit, label='logit')\n",
    "plt.plot(epoch_space, loss_logit_rank1, label='logit rank1')\n",
    "plt.plot(epoch_space, loss_logit_sigmoid, label='logit sigmoidal')\n",
    "plt.plot(epoch_space, loss_gumbel, label='concrete')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-D scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_latent_coordinates(train_loader_scatter_plot, model):\n",
    "    \n",
    "    x_coordinates = defaultdict(lambda: [])\n",
    "    y_coordinates = defaultdict(lambda: [])\n",
    "    \n",
    "    for batch_idx, (data, label) in enumerate(train_loader_scatter_plot):\n",
    "        _, z, _ = model(data)\n",
    "        index = label.data.cpu().numpy()[0]\n",
    "        \n",
    "        x_coordinates[index].append(z.data.cpu().numpy()[0][0])\n",
    "        y_coordinates[index].append(z.data.cpu().numpy()[0][1])\n",
    "        \n",
    "        if batch_idx == 10000:\n",
    "            break\n",
    "            \n",
    "    return x_coordinates, y_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_scatter_plot = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor())\n",
    "\n",
    "train_loader_scatter_plot = torch.utils.data.DataLoader(train_data_scatter_plot,\n",
    "                                           batch_size=1, shuffle=False, **{})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_latent_space_Gumbel, y_latent_space_Gumbel = find_latent_coordinates(train_loader_scatter_plot, VAE_gumbel)\n",
    "for label in x_latent_space_Gumbel:\n",
    "    plt.scatter(x_latent_space_Gumbel[label], y_latent_space_Gumbel[label], marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_latent_space_Gaussian, y_latent_space_Gaussian = find_latent_coordinates(train_loader_scatter_plot, VAE_Gaussian)\n",
    "for label in x_latent_space_Gaussian:\n",
    "    plt.scatter(x_latent_space_Gaussian[label], y_latent_space_Gaussian[label], marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_latent_space_logit, y_latent_space_logit = find_latent_coordinates(train_loader_scatter_plot, VAE_logit)\n",
    "for label in x_latent_space_logit:\n",
    "    plt.scatter(x_latent_space_logit[label], y_latent_space_logit[label], marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_latent_space_logit, y_latent_space_logit = find_latent_coordinates(train_loader_scatter_plot, VAE_logit_sigmoid)\n",
    "for label in x_latent_space_logit:\n",
    "    plt.scatter(x_latent_space_logit[label], y_latent_space_logit[label], marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_latent_space_logit, y_latent_space_logit = find_latent_coordinates(train_loader_scatter_plot, VAE_logit_rank1)\n",
    "for label in x_latent_space_logit:\n",
    "    plt.scatter(x_latent_space_logit[label], y_latent_space_logit[label], marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-D plot of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Let's check if the reconstructions make sense\n",
    "# Set model to test mode\n",
    "VAE_Gaussian.eval()\n",
    "    \n",
    "# Reconstructed\n",
    "train_data_plot = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor())\n",
    "\n",
    "train_loader_plot = torch.utils.data.DataLoader(train_data_plot,\n",
    "                                           batch_size=1, shuffle=False, **{})\n",
    "\n",
    "for batch_idx, (data, _) in enumerate(train_loader_plot):\n",
    "    x_hat, mu, logvar = VAE_Gaussian(data)\n",
    "    plt.imshow(x_hat.view(1,28,28).squeeze().data.numpy(), cmap='gray')\n",
    "    plt.title('%i' % train_data.train_labels[batch_idx])\n",
    "    plt.show()\n",
    "    if batch_idx == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms of z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_histogram(title, z):\n",
    "    hist_values = [[] for i in range(len(z))]\n",
    "    \n",
    "    for i in range(z[0].size(0)):\n",
    "        for j in range(len(z)):\n",
    "            hist_values[i].append(float(z[j][i]))\n",
    "        plt.hist(hist_values[i],  bins = 10, histtype=u'step')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_histogram('Histogram for Gaussian Distribution', z_Gaussian)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_histogram('Histogram for the Gumbel Distribution', z_gumbel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_histogram('Histogram for logit Distribution', z_logit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_histogram('Histogram for the Logit Distribution for Rank1 Covariance Approximation', z_logit_rank1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_histogram('Histogram for logit_sigmoid', z_logit_sigmoid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print ('KL mean for Gauss = ', torch.mean(torch.stack(KL_Gaussian)).data.cpu().numpy())\n",
    "print ('KL mean for Gumbel = ', torch.mean(torch.stack(KL_gumbel)).data.cpu().numpy())\n",
    "print ('KL mean for logit = ',torch.mean(torch.stack(KL_logit)).data.cpu().numpy())\n",
    "print ('KL mean for logit_rank1 = ', torch.mean(torch.stack(KL_logit_rank1)).data.cpu().numpy())\n",
    "print ('KL mean for logit_sigmoid = ', torch.mean(torch.stack(KL_logit_sigmoid)).data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print ('log_bern mean for Gaussian = ' ,torch.mean(torch.stack(log_bern_Gaussian)).data.cpu().numpy())\n",
    "print ('log_bern mean for Gumbel = ', torch.mean(torch.stack(log_bern_gumbel)).data.cpu().numpy())\n",
    "print ('log_bern mean for Logit = ', torch.mean(torch.stack(log_bern_logit)).data.cpu().numpy())\n",
    "print ('log_bern mean for Logit_rank1 = ', torch.mean(torch.stack(log_bern_rank1)).data.cpu().numpy())\n",
    "print ('log_bern mean for Logit_sigmoid = ', torch.mean(torch.stack(log_bern_sigmoid)).data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
